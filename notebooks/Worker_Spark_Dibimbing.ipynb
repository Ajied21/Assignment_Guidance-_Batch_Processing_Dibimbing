{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a867864e-ad0e-4175-9ba7-00028eba48a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import to_timestamp,col,when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4e293-7230-4d7c-bc4f-f8ba5d4e32e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ac2902-1ba2-4f59-b3db-da15788af820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv_path = Path('/resources/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba0f442-5349-4f76-87e6-7147a1963dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "postgres_host = os.getenv('POSTGRES_CONTAINER_NAME')\n",
    "postgres_dw_db = os.getenv('POSTGRES_DW_DB')\n",
    "postgres_user = os.getenv('POSTGRES_USER')\n",
    "postgres_password = os.getenv('POSTGRES_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025b23a6-c84c-48d4-87e4-94d4a9953d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparkcontext = pyspark.SparkContext.getOrCreate(conf=(\n",
    "        pyspark\n",
    "        .SparkConf()\n",
    "        .setAppName('Worker_Dibimbing')\n",
    "        .setMaster('local')\n",
    "        .set(\"spark.jars\", \"/opt/postgresql-42.2.18.jar\")\n",
    "    ))\n",
    "sparkcontext.setLogLevel(\"WARN\")\n",
    "\n",
    "spark = pyspark.sql.SparkSession(sparkcontext.getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc1fd6b-ed91-4843-af3a-14e7c6114b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dibimbing-jupyter:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Worker_Dibimbing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f885cf46920>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c585011-8fae-41bf-b5f6-ae412b029138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a3a7060-a06b-4c3f-9788-df1f27d5ca7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|customer_count|  status|\n",
      "+--------------+--------+\n",
      "|         89504| churned|\n",
      "|          9937|retained|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_timestamp, col, when, max, avg, sum, count, date_sub, lit\n",
    "from pyspark.sql.types import *\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = Path('/opt/app/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "postgres_host = os.getenv('POSTGRES_CONTAINER_NAME')\n",
    "postgres_dw_db = os.getenv('POSTGRES_DW_DB')\n",
    "postgres_user = os.getenv('POSTGRES_USER')\n",
    "postgres_password = os.getenv('POSTGRES_PASSWORD')\n",
    "\n",
    "# Spark configuration\n",
    "spark_host = \"spark://dibimbing-dataeng-spark-master:7077\"\n",
    "\n",
    "sparkcontext = pyspark.SparkContext.getOrCreate(conf=(\n",
    "        pyspark\n",
    "        .SparkConf()\n",
    "        .setAppName('Dibimbing')\n",
    "        .setMaster(spark_host)\n",
    "        .set(\"spark.jars\", \"/opt/bitnami/spark/jars/postgresql-42.2.18.jar\")\n",
    "    ))\n",
    "sparkcontext.setLogLevel(\"WARN\")\n",
    "\n",
    "spark = pyspark.sql.SparkSession(sparkcontext.getOrCreate())\n",
    "\n",
    "jdbc_url = f'jdbc:postgresql://{postgres_host}:5432/{postgres_dw_db}'\n",
    "jdbc_properties = {\n",
    "    'user': postgres_user,\n",
    "    'password': postgres_password,\n",
    "    'driver': 'org.postgresql.Driver',\n",
    "    'stringtype': 'unspecified'\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "olist_customers_dataset = spark.read.csv(\"/resources/data/olist/olist_customers_dataset.csv\", \n",
    "                                        header=True, \n",
    "                                        schema=StructType([\n",
    "                                        StructField(\"customer_id\", StringType(), True),\n",
    "                                        StructField(\"customer_unique_id\", StringType(), True),\n",
    "                                        StructField(\"customer_zip_code_prefix\", StringType(), True),\n",
    "                                        StructField(\"customer_city\", StringType(), True),\n",
    "                                        StructField(\"customer_state\", StringType(), True)\n",
    "                                        ])\n",
    "                                        )\n",
    "\n",
    "olist_orders_dataset = spark.read.csv(\"/resources/data/olist/olist_orders_dataset.csv\", \n",
    "                                            header=True, \n",
    "                                            schema=StructType([\n",
    "                                            StructField(\"order_id\", StringType(), True),\n",
    "                                            StructField(\"customer_id\", StringType(), True),\n",
    "                                            StructField(\"order_status\", StringType(), True),\n",
    "                                            StructField(\"order_purchase_timestamp\", TimestampType(), True),\n",
    "                                            StructField(\"order_approved_at\", TimestampType(), True),\n",
    "                                            StructField(\"order_delivered_carrier_date\", TimestampType(), True),\n",
    "                                            StructField(\"order_delivered_customer_date\", TimestampType(), True),\n",
    "                                            StructField(\"order_estimated_delivery_date\", TimestampType(), True)\n",
    "                                            ])\n",
    "                                            )\n",
    "\n",
    "olist_order_reviews_dataset = spark.read.csv(\"/resources/data/olist/olist_order_reviews_dataset.csv\", \n",
    "                                            header=True, \n",
    "                                            schema=StructType([\n",
    "                                            StructField(\"review_id\", StringType(), True),\n",
    "                                            StructField(\"order_id\", StringType(), True),\n",
    "                                            StructField(\"review_score\", IntegerType(), True),\n",
    "                                            StructField(\"review_comment_title\", StringType(), True),\n",
    "                                            StructField(\"review_comment_message\", StringType(), True),\n",
    "                                            StructField(\"review_creation_date\", TimestampType(), True),\n",
    "                                            StructField(\"review_answer_timestamp\", TimestampType(), True)\n",
    "                                            ])\n",
    "                                            )\n",
    "\n",
    "# Filter hanya pesanan yang sudah selesai untuk menghitung pembelian berulang\n",
    "completed_orders_df = olist_orders_dataset.filter(olist_orders_dataset.order_status == \"delivered\")\n",
    "\n",
    "# Gabungkan orders dengan customers berdasarkan customer_id\n",
    "orders_customers_df = completed_orders_df.join(olist_customers_dataset, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# Tentukan tanggal akhir data untuk menghitung churn (misalnya, 30 hari dari transaksi terakhir)\n",
    "end_date = datetime.strptime(\"2018-08-29\", \"%Y-%m-%d\")  # Misalkan data terakhir hingga 29 Agustus 2018\n",
    "lookback_period = 30  # Periode 30 hari sebagai cutoff untuk churn\n",
    "\n",
    "# Hitung tanggal terakhir pembelian setiap pelanggan\n",
    "last_purchase_df = orders_customers_df.groupBy(\"customer_unique_id\").agg(\n",
    "    max(\"order_purchase_timestamp\").alias(\"last_purchase_date\")\n",
    ")\n",
    "\n",
    "# Tandai pelanggan sebagai churn atau retained berdasarkan aktivitas dalam 30 hari terakhir\n",
    "churn_analysis_df = last_purchase_df.withColumn(\n",
    "    \"churned\",\n",
    "    when(col(\"last_purchase_date\") < date_sub(lit(end_date), lookback_period), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Gabungkan churn status ke tabel pelanggan untuk mengetahui churn-retention rate\n",
    "customers_churn_df = olist_customers_dataset.join(churn_analysis_df, on=\"customer_unique_id\", how=\"left\").fillna(0)\n",
    "\n",
    "# Hitung total pelanggan churn dan retained\n",
    "churn_summary = customers_churn_df.groupBy(\"churned\").agg(\n",
    "    count(\"customer_unique_id\").alias(\"customer_count\")\n",
    ")\n",
    "\n",
    "# Konversikan kolom churned menjadi kategori churned atau retained untuk kejelasan\n",
    "churn_summary = churn_summary.withColumn(\n",
    "    \"status\",\n",
    "    when(col(\"churned\") == 1, \"churned\").otherwise(\"retained\")\n",
    ").drop(\"churned\")\n",
    "\n",
    "# Tampilkan hasil churn-retention summary\n",
    "churn_summary.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa61a0-6585-47e2-943a-3a83bfc774aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
